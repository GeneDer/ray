model_loading_config:
  model_id: meta-llama/Llama-3.1-8B-Instruct

accelerator_type: A10G

engine_kwargs:
  trust_remote_code: true
  tokenizer_pool_size: 2
  tokenizer_pool_extra_config:
    runtime_env:
      pip: null
    # Llama 3.1's context length is 128k, but we set a lower one to avoid GPU OOM.
  max_model_len: 8192
  max_num_batched_tokens: 8192
  enable_chunked_prefill: false
  tensor_parallel_size: 1
